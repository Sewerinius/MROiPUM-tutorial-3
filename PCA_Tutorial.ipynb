{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8Tr7VRJloT6"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import LeaveOneOut, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mean, stdev\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import image, pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIuEexKbEv7M"
   },
   "source": [
    "# PCA\n",
    "1. Zbiory danych użyte w ćwiczeniu:\n",
    "  * 10-klasowy MNIST\n",
    "  * 2-klasowy eeg-eye-state (własny)\n",
    "  * własny obrazek 200x200 pikseli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Yush2CXmGfw"
   },
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\", data_home=\"./mnist_784\", cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9RhnsKQSoyw5"
   },
   "outputs": [],
   "source": [
    "ees = fetch_openml(\"eeg-eye-state\", data_home=\"./eeg-eye-state\", cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4TvMOFvE0V2"
   },
   "source": [
    "2. W ćwiczeniu wykorzystamy zbiory testowe stanowiące 10, 25, 75% zbiory danych (po 10 instancji dla każdego przypadku) oraz one-to-all (LeaveOneOut). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCS2fwWgo59u"
   },
   "source": [
    "# KNN z błędem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtLYE3F5mQue"
   },
   "outputs": [],
   "source": [
    "def error_for_k(dataset, k, train_size, random_seed):\n",
    "\n",
    "    # dzielimy dane na treningowe i testowe\n",
    "    data = dataset.data\n",
    "    labels = dataset.target\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size=train_size, random_state=random_seed)\n",
    "    \n",
    "    # trenujemy klasyfikator\n",
    "    knn_all = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn_all.fit(train_data, train_labels)\n",
    "\n",
    "    prediction = knn_all.predict(test_data)\n",
    "    # liczymy błąd jako średnią ilości k najbliższych punktów uczących do badanego punktu testowego dzielone przez lczbę k\n",
    "    dist, indices  = knn_all.kneighbors(test_data)\n",
    "    d = pd.DataFrame({'prediction': prediction, 'indices': list(indices), 'actual': test_labels})\n",
    "    d['error'] = d.apply(lambda x: np.count_nonzero(train_labels[x['indices']] == x['actual']) / k, axis=1)\n",
    "    return d['error'].mean()\n",
    "\n",
    "def get_score(dataset, k, train_size):\n",
    "    # mierzymy błąd dla 10 powtórzeń\n",
    "    errors = [error_for_k(dataset, k, train_size, i) for i in range(10)]\n",
    "    mean_error = mean(errors)\n",
    "    mean_stdev = stdev(errors)\n",
    "    score = (k, train_size, mean_error, mean_stdev)\n",
    "    print(score)\n",
    "    return score\n",
    "\n",
    "samples = 2000 # bo dla większej ilości za długo się liczy\n",
    "def get_mean_and_stdev(dataset, samples=2000):\n",
    "    d2 = cp.copy(dataset)\n",
    "    d2.data, d2.target = dataset.data[:samples], dataset.target[:samples]\n",
    "    return [get_score(d2, k, train_size) for k in [1, 3, 5, 100] for train_size in [0.05, 0.25, 0.75]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "r2WjqaKKpfO_",
    "outputId": "5ceafe43-5386-423c-cec4-55e844e2a251"
   },
   "outputs": [],
   "source": [
    "print(\"MNIST 10 klas\")\n",
    "print(\"k, train_size, mean_error, mean_stdev\")\n",
    "tmp = get_mean_and_stdev(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "AQ0ARM6XL2gC",
    "outputId": "0313a51c-5485-4e07-d7fc-029bdd8b9703"
   },
   "outputs": [],
   "source": [
    "print(\"Zbiór własny 2 klasy\")\n",
    "print(\"k, train_size, mean_error, mean_stdev\")\n",
    "tmp = get_mean_and_stdev(ees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "LA-U64N82HrN",
    "outputId": "e7628dc8-5d9b-4152-cd9f-a2e96773d371"
   },
   "outputs": [],
   "source": [
    "# Poniżej wyniki z użyciem splittera LeaveOneOut\n",
    "\n",
    "def build_result_df(train_data, test_data, k, train_labels, test_labels):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(train_data, train_labels)\n",
    "    \n",
    "    prediction = knn.predict(test_data)\n",
    "    dist, ind = knn.kneighbors(test_data)\n",
    "    d = pd.DataFrame({'prediction': prediction, 'ind': list(ind), 'actual': test_labels})\n",
    "    d['error'] = d.apply(lambda x: np.count_nonzero(train_labels[x['ind']] == x['actual']) / k, axis=1)\n",
    "    return d\n",
    "\n",
    "def leave_one_out_kneighbors(dataset, samples, k):\n",
    "    splitter = LeaveOneOut()\n",
    "\n",
    "    data_sample = dataset.data[:samples]\n",
    "    labels_sample = dataset.target[:samples]\n",
    "\n",
    "    for train_indices, test_indices in splitter.split(data_sample):\n",
    "        train_data = data_sample[train_indices]\n",
    "        test_data = data_sample[test_indices]\n",
    "\n",
    "        train_labels = labels_sample[train_indices]\n",
    "        test_labels = labels_sample[test_indices]\n",
    "        \n",
    "        yield build_result_df(train_data, test_data, k, train_labels, test_labels)\n",
    "        \n",
    "samples = 200 # zmniejszamy, ponieważ liczenie trwa długo dla 2000 próbek\n",
    "ks = [1,3,5,10] # ilość sąsiadów\n",
    "\n",
    "def get_errors_for_leaveoneout(dataset, samples, k):\n",
    "    print(\"Using %s-nearest-neighbors classifier.:\" % k)\n",
    "    mean= np.mean(np.array([d['error'].mean() for d in leave_one_out_kneighbors(dataset, samples, k)]))\n",
    "    print(\"mean: \", mean)\n",
    "\n",
    "print(\"zbiór MNIST 10-klasowy (leave one out split):\")\n",
    "for k in ks: \n",
    "    get_errors_for_leaveoneout(mnist, samples, k)\n",
    "\n",
    "print(\"zbiór 2-klasowy (leave one out split):\")\n",
    "for k in ks:\n",
    "    get_errors_for_leaveoneout(ees, samples, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gzc7LJMa42vO"
   },
   "source": [
    "Zadanie 1.:\n",
    "\n",
    "Dla zbiorów danych wielowymiarowych zastosować klasyfikator k-NN dla każdych 10 instancji, policzyć średni błąd klasyfikacji oraz jego odchylenie standardowe, dla k=1,3,5,100. Zbiór uczący: 5% , 25% , 75% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5JgR983t4lt3"
   },
   "source": [
    "Dla jakiego k otrzymujemy najmniejszy błąd klasyfikacji oraz najmniejsze odchylenie standardowe (wariancję) dla poszczególnych przypadków uczenia i zbiorów danych? Dlaczego? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9JWATSW9515H"
   },
   "source": [
    "Zbiór MNIST\n",
    "\n",
    "| % zbioru testowego | k   | średni błąd | odchylenie standardowe |   |\n",
    "|--------------------|-----|-------------|------------------------|---|\n",
    "| 5%                 | 1   |             |                        |   |\n",
    "| 5%                 | 3   |             |                        |   |\n",
    "| 5%                 | 5   |             |                        |   |\n",
    "| 5%                 | 100 |             |                        |   |\n",
    "| 25%                | 1   |             |                        |   |\n",
    "| 25%                | 3   |             |                        |   |\n",
    "| 25%                | 5   |             |                        |   |\n",
    "| 25%                | 100 |             |                        |   |\n",
    "| 75%                | 1   |             |                        |   |\n",
    "| 75%                | 3   |             |                        |   |\n",
    "| 75%                | 5   |             |                        |   |\n",
    "| 75%                | 100 |             |                        |   |\n",
    "\n",
    "Zbiór własny\n",
    "\n",
    "| % zbioru testowego | k   | średni błąd | odchylenie standardowe |   |\n",
    "|--------------------|-----|-------------|------------------------|---|\n",
    "| 5%                 | 1   |             |                        |   |\n",
    "| 5%                 | 3   |             |                        |   |\n",
    "| 5%                 | 5   |             |                        |   |\n",
    "| 5%                 | 100 |             |                        |   |\n",
    "| 25%                | 1   |             |                        |   |\n",
    "| 25%                | 3   |             |                        |   |\n",
    "| 25%                | 5   |             |                        |   |\n",
    "| 25%                | 100 |             |                        |   |\n",
    "| 75%                | 1   |             |                        |   |\n",
    "| 75%                | 3   |             |                        |   |\n",
    "| 75%                | 5   |             |                        |   |\n",
    "| 75%                | 100 |             |                        |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QVhjcLmI6a7o"
   },
   "source": [
    "# Wizualizacja PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZAqWrYDd6qSX"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "data = ees.data\n",
    "target = ees.target\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(data, target, test_size = 0.05)\n",
    "scaler.fit(train_x)\n",
    "\n",
    "#normalizacja cech\n",
    "train_x = scaler.transform(train_x) \n",
    "test_x = scaler.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "Ph0wseec6rG_",
    "outputId": "55ebb9ce-95a3-4824-d7b6-40da814f9e31"
   },
   "outputs": [],
   "source": [
    "def remove_outliers(x, y, m=2): \n",
    "    #Usuwamy te rzędy, w których co najmniej jedna współrzędna jest oddalona od średniej o więcej niż m * wartość_standardowa\n",
    "    not_outliers = np.all(abs(x - np.mean(x)) < m * np.std(x), axis=1)\n",
    "    return x[not_outliers], y[not_outliers]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "reduced_x_with_outliers = pca.fit_transform(train_x)\n",
    "reduced_x, train_y_2 = remove_outliers(reduced_x_with_outliers, train_y, m=20)\n",
    "a_indices = train_y_2 == '1'\n",
    "b_indices = train_y_2 != '1'\n",
    "\n",
    "#rysujemy na niebiesko / zielono wszystkie punkty treningowe\n",
    "plt.scatter(reduced_x[a_indices, 0], reduced_x[a_indices, 1], color = \"green\")\n",
    "plt.scatter(reduced_x[b_indices, 0], reduced_x[b_indices, 1], color = \"blue\")\n",
    "\n",
    "best_k = 3 # tutaj nalezy podać najlepsze k z poprzedniego zadania dla własnego zbioru danych\n",
    "knn = KNeighborsClassifier(n_neighbors = best_k)\n",
    "knn.fit(reduced_x_with_outliers, train_y)\n",
    "red_test_x = pca.transform(test_x)\n",
    "pred_y = knn.predict(red_test_x)\n",
    "\n",
    "#rysujemy na czerwono wszystkie błednie określone punkty\n",
    "wrong_i = pred_y != test_y\n",
    "plt.scatter(red_test_x[wrong_i, 0], red_test_x[wrong_i, 1], color = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g2clQ09l7BJH"
   },
   "source": [
    "Zadanie 2.:\n",
    "\n",
    "Zwizualizować przy pomocy PCA wyniki dla zbioru własnego, zakładając że zbiór uczący to 5% oraz 75% oraz najlepsze k wybrane w punkcie 3. Porównać do PCA w którym znana jest przynależność do klas 100% punktów. Przedyskutować wynik. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfEoanzkt3sz"
   },
   "source": [
    "# KNeighborsRegressor dla kolorowego obrazka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lkmWmFDfFjCt"
   },
   "source": [
    "Należy wygenerować dla obrazka zbiór pikseli uczących stanowiących pewien % oryginalnego obrazka. Wartości pozostałych pikseli zostaną obliczone jako średnia z najbliższych k sąsiadów ze zbioru uczącego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-EU8jxeKeYuH"
   },
   "outputs": [],
   "source": [
    "# 5. Wczytujemy obrazek w oryginalnym rozmiarze.\n",
    "cats = image.imread('aoshima-cats.jpg') # 900x900\n",
    "cats = cv2.resize(cats, (200,200)) # scale to 200x200\n",
    "pyplot.imshow(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "22WbkniMeboT"
   },
   "outputs": [],
   "source": [
    "DIM = 200 # wymiar N badanego obrazka\n",
    "p = .5 # % zbioru testowego\n",
    "k = 10 # ile sąsiadów\n",
    "\n",
    "# tworzymy tablicę indeksów (N^2 x 2)\n",
    "indices_flat = np.array([(x,y) for x in range(DIM) for y in range(DIM)])\n",
    "\n",
    "# spłaszczamy obrazek do tego samego wymiaru (N^2 x 3 rgb)\n",
    "cats_flat = np.reshape(cats, (DIM*DIM, -1))\n",
    "\n",
    "# wybór zbioru pikseli i indeksów trenujących i testowych\n",
    "train_c, test_c, train_i, test_i = train_test_split(cats_flat, indices_flat, test_size=p)\n",
    "\n",
    "# korzystamy z metody najbliższych sąsiadów i uczymy model\n",
    "knr = KNeighborsRegressor(n_neighbors=k)\n",
    "knr.fit(train_i, train_c)\n",
    "\n",
    "# predykcja kolorów pozostałych pixeli (zaokrąglamy do liczby całkowitej z przedziału [0..255])\n",
    "predict_c = knr.predict(test_i).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BioLtLm4tCop"
   },
   "outputs": [],
   "source": [
    "# łączymy piksele oryginalne z tymi z predykcji\n",
    "pixels = np.concatenate((train_c, predict_c))\n",
    "indices = np.concatenate((train_i, test_i))\n",
    "indices_flattened = np.array([x[0]*DIM + x[1] for x in indices])\n",
    "\n",
    "result_image = pixels[np.argsort(indices_flattened)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJEhWgJ4GG-K"
   },
   "source": [
    "Jak wyglądają wygenerowane obrazki? Możemy sprawdzić jak modyfikacje parametrów p i k wpływają na finalny rezultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vo7Hez_6egCp"
   },
   "outputs": [],
   "source": [
    "# uzyskany obrazek\n",
    "def reshape_and_display(image, DIM=200):\n",
    "    pyplot.imshow(image.reshape((DIM, DIM, -1)))\n",
    "\n",
    "reshape_and_display(result_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxId_nb3GSfF"
   },
   "source": [
    "Do oszacowania błędu wykorzystujemy średni błąd kwadratowy wartości pikseli RGB w porównaniu z oryginałem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihh_st_Mu1em"
   },
   "outputs": [],
   "source": [
    "# 6. błąd w porównaniu z oryginałem \n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(cats_flat, result_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Df-sYtHCGjKM"
   },
   "source": [
    "Następnie, powtarzamy powyższą procedurę kilkukrotnie (Q=5) i generujemy obrazek, którego wartość pikseli obliczamy jako średnią z wygenerowanych Q obrazków. Sprawdzamy czy zmniejszył się błąd. Jak teraz wygląda obrazek?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNsIQnDYvAiI"
   },
   "outputs": [],
   "source": [
    "# 7. wykonujemy kilka prób i generujemy obrazek będący średnią z wartości pixeli\n",
    "def predicted_image(original_image, p, k,DIM=200):\n",
    "    indices_flat = np.array([(x,y) for x in range(DIM) for y in range(DIM)])\n",
    "    image_flat = np.reshape(original_image, (DIM*DIM, -1))\n",
    "    \n",
    "    train_c, test_c, train_i, test_i = train_test_split(image_flat, indices_flat, test_size=p)\n",
    "    \n",
    "    knr = KNeighborsRegressor(n_neighbors=k)\n",
    "    knr.fit(train_i, train_c)\n",
    "    predict_c = knr.predict(test_i).astype(np.int32)\n",
    "    \n",
    "    pixels = np.concatenate((train_c, predict_c))\n",
    "    indices = np.concatenate((train_i, test_i))\n",
    "    indices_flattened = np.array([x[0]*DIM + x[1] for x in indices])\n",
    "\n",
    "    return pixels[np.argsort(indices_flattened)]\n",
    "    \n",
    "Q = 5 # ilość prób\n",
    "predicted_images = np.array([predicted_image(cats, p, k) for _ in range(Q)])\n",
    "mean_image = np.mean(predicted_images, axis=0, dtype='uint64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IYIiqQMuvGjb"
   },
   "outputs": [],
   "source": [
    "# obrazek powstały ze średniej\n",
    "reshape_and_display(mean_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ntjJ-g4vK_-"
   },
   "outputs": [],
   "source": [
    "# Czy bład się zmniejszył? otóż tak\n",
    "mean_squared_error(cats_flat, mean_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpfxKWNVy6ee"
   },
   "source": [
    "Zadanie 3.:\n",
    "\n",
    "a. \n",
    "Dla obrazka wygenerować losowo zbiór pikseli uczących w ilości 10%, 50%, 75%. Zrobić to 5- krotnie (5\n",
    "instancji dla każdego zbioru uczącego). Kolory reszty z pikseli policzyć jako średnią z k=1, 3, 10\n",
    "najbliższych sąsiadów ze zbioru uczącego. Jak wyglądają wygenerowane obrazki? (Podpowiedź: można użyć funkcji predicted_image oraz reshape_and_display, odpowiednio sparametryzowanej) \n",
    "\n",
    "b.\n",
    "Znaleźć błąd dla każdego wygenerowanego obrazka w porównaniu z oryginałem. \n",
    "(można użyć funkcji mean_squared_error i wykorzystać tabelkę poniżej na wyniki)\n",
    "\n",
    "c.\n",
    "Dla każdego przypadku uczącego (10%, 50%, 75%.) policzyć średnie wartości pikseli z wygenerowanych 5 obrazków. Jak wygląda tak wygenerowany obrazek. Czy błąd się\n",
    "zmniejszył??? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Olt0i-ZozD9w"
   },
   "source": [
    "#### Błędy dla podpunktu 3.b\n",
    "\n",
    "|                    | k najbliższych sąsiadów |     |      |   |\n",
    "|:------------------:|:-----------------------:|:---:|:----:|:-:|\n",
    "| % zbioru testowego |           k=1           | k=3 | k=10 |   |\n",
    "|         10%        |                         |     |      |   |\n",
    "|         50%        |                         |     |      |   |\n",
    "|         75%        |                         |     |      |   |\n",
    "\n",
    "\n",
    "#### Błędy dla podpunktu 3.c\n",
    "\n",
    "|                    | k najbliższych sąsiadów |     |      |   |\n",
    "|:------------------:|:-----------------------:|:---:|:----:|:-:|\n",
    "| % zbioru testowego |           k=1           | k=3 | k=10 |   |\n",
    "|         10%        |                         |     |      |   |\n",
    "|         50%        |                         |     |      |   |\n",
    "|         75%        |                         |     |      |   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1beKO-fWHRPk"
   },
   "outputs": [],
   "source": [
    "# MIEJSCE NA KOD DO ZADANIA 3.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PCA_Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
